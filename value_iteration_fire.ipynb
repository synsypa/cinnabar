{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_fire\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FireLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0 : Left\n",
    "1 : Down\n",
    "2 : Right\n",
    "3 : Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 5, -1, True)],\n",
       " 1: [(1.0, 5, -1, True)],\n",
       " 2: [(1.0, 5, -1, True)],\n",
       " 3: [(1.0, 5, -1, True)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_value(env, discount=1.0, max_iters=10000, convergence=1e-20):\n",
    "    \n",
    "    # initialize reward value for each state\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # for convergence comparison\n",
    "        old_V = np.copy(V) \n",
    "        \n",
    "        # calculate reward value for each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            state_rewards = []\n",
    "            for action in range(env.action_space.n):\n",
    "                expected_rewards = []\n",
    "                # calculate total expected reward for each action\n",
    "                for next_state_action in env.P[state][action]: \n",
    "                    transition_p, next_state, reward_p, _ = next_state_action \n",
    "                    expected_rewards.append((transition_p * (reward_p + discount * V[next_state]))) \n",
    "                \n",
    "                state_rewards.append(np.sum(expected_rewards))\n",
    "            \n",
    "            # update value table with reward for action with the best reward\n",
    "            V[state] = max(state_rewards) \n",
    "            \n",
    "        # check convergence\n",
    "        if (np.sum(np.fabs(V - old_V)) <= convergence):\n",
    "            print(f'Converged on iteration {i}')\n",
    "            return(V)\n",
    "    \n",
    "    print(f'Did not converge in {i} iterations, returning last value table')\n",
    "    return(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(value_array, discount=1.0):\n",
    " \n",
    "    # initialize policy\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        # initialize the rewards for a state\n",
    "        V = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # compute expected reward for all actions of state\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_state_action in env.P[state][action]: \n",
    "                transition_p, next_state, reward_p, _ = next_state_action \n",
    "                V[action] += (transition_p * (reward_p + discount * value_array[next_state]))\n",
    "        \n",
    "        # select the action which has max reward as an optimal action of the state\n",
    "        policy[state] = np.argmax(V)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged on iteration 29\n"
     ]
    }
   ],
   "source": [
    "value_array =  iterate_value(env, discount=discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00444109, -0.00447778, -0.00481448, -0.00848145, -0.00437416,\n",
       "       -1.33333333, -0.10972812, -1.33333333, -0.00367469,  0.00395261,\n",
       "        0.02141034, -1.33333333, -1.33333333,  0.02969566,  0.36269961,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_policy(value_array, discount=discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(env, policy):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    reward = 0\n",
    "    state = 0\n",
    "    step = 0\n",
    "    dead = False\n",
    "    while not dead:\n",
    "        next = env.step(int(policy[state]))\n",
    "        dead = next[2]\n",
    "        reward += next[1]\n",
    "        state = next[0]\n",
    "        step += 1\n",
    "        print(f'Step reward: {next[1]}')\n",
    "        print(f'Cumulative reward: {reward}')\n",
    "        env.render()\n",
    "\n",
    "    if reward > 0:\n",
    "        print(f'Gotem in {step} steps with reward {reward}')\n",
    "    else:\n",
    "        print(f'RIP in {step} steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.01\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.02\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.03\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.03\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.03\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.03\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.04\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.05\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.060000000000000005\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.060000000000000005\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.060000000000000005\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.07\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.07\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.08\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.09\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "Step reward: 0.0\n",
      "Cumulative reward: -0.09\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.09999999999999999\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Step reward: -0.01\n",
      "Cumulative reward: -0.10999999999999999\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Step reward: 1.0\n",
      "Cumulative reward: 0.89\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Gotem in 25 steps with reward 0.89\n"
     ]
    }
   ],
   "source": [
    "run_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
