{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0 : Left\n",
    "1 : Down\n",
    "2 : Right\n",
    "3 : Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_value(env, discount=1.0, max_iters=10000, convergence=1e-20):\n",
    "    \n",
    "    # initialize reward value for each state\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # for convergence comparison\n",
    "        old_V = np.copy(V) \n",
    "        \n",
    "        # calculate reward value for each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            state_rewards = []\n",
    "            for action in range(env.action_space.n):\n",
    "                expected_rewards = []\n",
    "                # calculate total expected reward for each action\n",
    "                for next_state_action in env.env.P[state][action]: \n",
    "                    transition_p, next_state, reward_p, _ = next_state_action \n",
    "                    expected_rewards.append((transition_p * (reward_p + discount * V[next_state]))) \n",
    "                \n",
    "                state_rewards.append(np.sum(expected_rewards))\n",
    "            \n",
    "            # update value table with reward for action with the best reward\n",
    "            V[state] = max(state_rewards) \n",
    "            \n",
    "        # check convergence\n",
    "        if (np.sum(np.fabs(V - old_V)) <= convergence):\n",
    "            print(f'Converged on iteration {i}')\n",
    "            return(V)\n",
    "    \n",
    "    print(f'Did not converge in {i} iterations, returning last value table')\n",
    "    return(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(value_array, discount=1.0):\n",
    " \n",
    "    # initialize policy\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        # initialize the rewards for a state\n",
    "        V = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # compute expected reward for all actions of state\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_state_action in env.env.P[state][action]: \n",
    "                transition_p, next_state, reward_p, _ = next_state_action \n",
    "                V[action] += (transition_p * (reward_p + discount * value_array[next_state]))\n",
    "        \n",
    "        # select the action which has max reward as an optimal action of the state\n",
    "        policy[state] = np.argmax(V)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged on iteration 105\n"
     ]
    }
   ],
   "source": [
    "value_array =  iterate_value(env, discount=discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01543434, 0.0155907 , 0.0274401 , 0.01568006],\n",
       "       [0.02685373, 0.        , 0.05978021, 0.        ],\n",
       "       [0.05841341, 0.13378315, 0.1967357 , 0.        ],\n",
       "       [0.        , 0.2465377 , 0.54419553, 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_array.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_policy(value_array, discount=discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(env, policy):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    reward = 0\n",
    "    state = 0\n",
    "    step = 0\n",
    "    dead = False\n",
    "    while not dead and reward == 0:\n",
    "        next = env.step(int(policy[state]))\n",
    "        dead = next[2]\n",
    "        reward += next[1]\n",
    "        state = next[0]\n",
    "        step += 1\n",
    "        env.render()\n",
    "    \n",
    "    if reward == 1:\n",
    "        print(f'Gotem in {step} steps')\n",
    "    else: \n",
    "        print(f'RIP in {step} steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
